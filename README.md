# [ICLR2024] Multi-Scale Representations by Varying Window Attention for Semantic Segmentation

<!-- [[`Code`](https://github.com/yan-hao-tian/lawin/blob/70903a10403d4d8b87b0a2fe39a7cf045cf5a476/mask_former/modeling/heads/pixel_decoder.py#L196)] -->

ğŸ”¥ğŸ”¥ğŸ”¥ [ICLR2024 Poster](https://openreview.net/forum?id=lAhWGOkpSR) ğŸ”¥ğŸ”¥ğŸ”¥

[`arxiv`](https://arxiv.org/abs/2404.16573)

[`HuggingFace`](https://huggingface.co/yan-hao-tian)ğŸ¤—

## Running VW
[VW-Swin/ConvNeXt](mmsegmentation-custom)

[VW-MaskFormer](MaskFormer)

[VW-Mask2Former](Mask2Former)


## <a name="CitingMaskFormer"></a>Citing VW

<!-- If you use VWA or VWFormer in your research or wish to refer to the baseline results published in the [Model Zoo]((#ModelZoo)), please use the following BibTeX entry. -->

```BibTeX
@inproceedings{yan2023multi,
  title={Multi-Scale Representations by Varing Window Attention for Semantic Segmentation},
  author={Yan, Haotian and Wu, Ming and Zhang, Chuang},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
```
## ğŸ™ğŸ™ğŸ™
I am looking for some intern opportunity on Vision or Multi-Modal, best based in Beijing (other cities will be ok but I need to persuade my tutor and GF). If you need an intern, please contact me via yanhaotian@bupt.edu.cn or leave your link. BTW I will obtain Ph.D degree in September 2025.
